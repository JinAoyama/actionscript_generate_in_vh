{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCSscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(a, b):\n",
    "    dp = [[0] * (len(b)+1) for i in range(len(a)+1)]\n",
    "\n",
    "    for i, vi in enumerate(a):\n",
    "        for j, vj in enumerate(b):\n",
    "            if vi == vj:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
    "\n",
    "    return dp[len(a)][len(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VH dataset (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: \n",
      "LCSscore(Action): 0.505\n",
      "LCSscore(Object): 0.114\n",
      "LCSscore(Step): 0.092\n",
      "LCSscore(Mean): 0.237\n",
      "\n",
      "Scene 2: \n",
      "LCSscore(Action): 0.504\n",
      "LCSscore(Object): 0.119\n",
      "LCSscore(Step): 0.096\n",
      "LCSscore(Mean): 0.240\n",
      "\n",
      "Scene 3: \n",
      "LCSscore(Action): 0.486\n",
      "LCSscore(Object): 0.093\n",
      "LCSscore(Step): 0.075\n",
      "LCSscore(Mean): 0.218\n",
      "\n",
      "Scene 4: \n",
      "LCSscore(Action): 0.479\n",
      "LCSscore(Object): 0.148\n",
      "LCSscore(Step): 0.119\n",
      "LCSscore(Mean): 0.248\n",
      "\n",
      "Scene 5: \n",
      "LCSscore(Action): 0.479\n",
      "LCSscore(Object): 0.094\n",
      "LCSscore(Step): 0.076\n",
      "LCSscore(Mean): 0.216\n",
      "\n",
      "Scene 6: \n",
      "LCSscore(Action): 0.496\n",
      "LCSscore(Object): 0.116\n",
      "LCSscore(Step): 0.093\n",
      "LCSscore(Mean): 0.235\n",
      "\n",
      "Scene 7: \n",
      "LCSscore(Action): 0.501\n",
      "LCSscore(Object): 0.146\n",
      "LCSscore(Step): 0.112\n",
      "LCSscore(Mean): 0.253\n",
      "\n",
      "Mean: \n",
      "LCSscore(Action): 0.493\n",
      "LCSscore(Object): 0.119\n",
      "LCSscore(Step): 0.095\n",
      "LCSscore(Mean): 0.235\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_mean = []\n",
    "object_mean = []\n",
    "step_mean = []\n",
    "mean_mean = []\n",
    "\n",
    "for scene_num in range(1, 8):\n",
    "\n",
    "    with open(f\"generated_vh_data/Object_name/object_name_script_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the generated data file\n",
    "        result_scripts = json.load(json_file)\n",
    "\n",
    "    with open(f\"correct_vh_data/correct_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the correct data file\n",
    "        correct_data = json.load(json_file)\n",
    "\n",
    "    step_score_list = []\n",
    "    action_score_list = []\n",
    "    object_score_list = []\n",
    "\n",
    "    for f_num, correct in correct_data.items():\n",
    "\n",
    "        evaluation_data = result_scripts.get(f_num)\n",
    "\n",
    "        for i, data in enumerate(evaluation_data):\n",
    "        \n",
    "            data = data.split()\n",
    "            new_data = [str(data[1])[1:-1]]\n",
    "\n",
    "            if len(data) == 4:\n",
    "                new_data.append(str(data[2])[1:-1]) \n",
    "            elif len(data) == 6:\n",
    "                new_data.append(str(data[2])[1:-1] + \"&\" + str(data[4])[1:-1])\n",
    "\n",
    "            evaluation_data[i] = new_data\n",
    "\n",
    "        for i, data in enumerate(correct):\n",
    "\n",
    "            data = data.split()\n",
    "            new_data = [str(data[0])[1:-1].lower()]\n",
    "\n",
    "            if len(data) == 3:\n",
    "                new_data.append(str(data[1])[1:-1]) \n",
    "            elif len(data) == 5:\n",
    "                new_data.append(str(data[1])[1:-1] + \"&\" + str(data[3])[1:-1])\n",
    "\n",
    "            correct[i] = new_data\n",
    "\n",
    "        step_score_list.append(lcs([' '.join(row) for row in evaluation_data], [' '.join(row) for row in correct]) / max(len(evaluation_data), len(correct)))\n",
    "        action_score_list.append(lcs([row[0] for row in evaluation_data], [row[0] for row in correct]) / max(len([row[0] for row in evaluation_data]), len([row[0] for row in correct])))\n",
    "        object_score_list.append(lcs([row[1] for row in evaluation_data if len(row) > 1], [row[1] for row in correct if len(row) > 1]) / max(len([row[1] for row in evaluation_data if len(row) > 1]), len([row[1] for row in correct if len(row) > 1])))\n",
    "\n",
    "    print(f\"Scene {scene_num}: \")\n",
    "    print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_score_list)))\n",
    "    print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_score_list)))\n",
    "    print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_score_list)))\n",
    "    print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])))\n",
    "    print()\n",
    "\n",
    "    action_mean.append(numpy.mean(action_score_list))\n",
    "    object_mean.append(numpy.mean(object_score_list))\n",
    "    step_mean.append(numpy.mean(step_score_list))\n",
    "    mean_mean.append([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])\n",
    "\n",
    "print(\"Mean: \")\n",
    "print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_mean)))\n",
    "print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_mean)))\n",
    "print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_mean)))\n",
    "print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean(mean_mean)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VH dataset (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: \n",
      "LCSscore(Action): 0.505\n",
      "LCSscore(Object): 0.501\n",
      "LCSscore(Step): 0.436\n",
      "LCSscore(Mean): 0.480\n",
      "\n",
      "Scene 2: \n",
      "LCSscore(Action): 0.504\n",
      "LCSscore(Object): 0.492\n",
      "LCSscore(Step): 0.424\n",
      "LCSscore(Mean): 0.473\n",
      "\n",
      "Scene 3: \n",
      "LCSscore(Action): 0.486\n",
      "LCSscore(Object): 0.472\n",
      "LCSscore(Step): 0.408\n",
      "LCSscore(Mean): 0.455\n",
      "\n",
      "Scene 4: \n",
      "LCSscore(Action): 0.479\n",
      "LCSscore(Object): 0.454\n",
      "LCSscore(Step): 0.392\n",
      "LCSscore(Mean): 0.442\n",
      "\n",
      "Scene 5: \n",
      "LCSscore(Action): 0.479\n",
      "LCSscore(Object): 0.459\n",
      "LCSscore(Step): 0.404\n",
      "LCSscore(Mean): 0.447\n",
      "\n",
      "Scene 6: \n",
      "LCSscore(Action): 0.496\n",
      "LCSscore(Object): 0.475\n",
      "LCSscore(Step): 0.418\n",
      "LCSscore(Mean): 0.463\n",
      "\n",
      "Scene 7: \n",
      "LCSscore(Action): 0.501\n",
      "LCSscore(Object): 0.488\n",
      "LCSscore(Step): 0.423\n",
      "LCSscore(Mean): 0.471\n",
      "\n",
      "Mean: \n",
      "LCSscore(Action): 0.493\n",
      "LCSscore(Object): 0.477\n",
      "LCSscore(Step): 0.415\n",
      "LCSscore(Mean): 0.462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_mean = []\n",
    "object_mean = []\n",
    "step_mean = []\n",
    "mean_mean = []\n",
    "\n",
    "for scene_num in range(1, 8):\n",
    "\n",
    "    with open(f\"generated_vh_data/Class_name/class_name_script_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the generated data file\n",
    "        result_scripts = json.load(json_file)\n",
    "\n",
    "    with open(f\"correct_vh_data/correct_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the correct data file\n",
    "        correct_data = json.load(json_file)\n",
    "\n",
    "    step_score_list = []\n",
    "    action_score_list = []\n",
    "    object_score_list = []\n",
    "\n",
    "    for f_num, correct in correct_data.items():\n",
    "\n",
    "        evaluation_data = result_scripts.get(f_num)\n",
    "\n",
    "        for i, data in enumerate(evaluation_data):\n",
    "        \n",
    "            data = data.split()\n",
    "            new_data = [str(data[1])[1:-1]]\n",
    "\n",
    "            if len(data) == 4:\n",
    "                new_data.append(str(data[2])[1:-1]) \n",
    "            elif len(data) == 6:\n",
    "                new_data.append(str(data[2])[1:-1] + \"&\" + str(data[4])[1:-1])\n",
    "\n",
    "            evaluation_data[i] = new_data\n",
    "\n",
    "        for i, data in enumerate(correct):\n",
    "\n",
    "            data = data.split()\n",
    "            new_data = [str(data[0])[1:-1].lower()]\n",
    "\n",
    "            if len(data) == 3:\n",
    "                new_data.append(str(data[1])[1:-1]) \n",
    "            elif len(data) == 5:\n",
    "                new_data.append(str(data[1])[1:-1] + \"&\" + str(data[3])[1:-1])\n",
    "\n",
    "            correct[i] = new_data\n",
    "\n",
    "        step_score_list.append(lcs([' '.join(row) for row in evaluation_data], [' '.join(row) for row in correct]) / max(len(evaluation_data), len(correct)))\n",
    "        action_score_list.append(lcs([row[0] for row in evaluation_data], [row[0] for row in correct]) / max(len([row[0] for row in evaluation_data]), len([row[0] for row in correct])))\n",
    "        object_score_list.append(lcs([row[1] for row in evaluation_data if len(row) > 1], [row[1] for row in correct if len(row) > 1]) / max(len([row[1] for row in evaluation_data if len(row) > 1]), len([row[1] for row in correct if len(row) > 1])))\n",
    "\n",
    "    print(f\"Scene {scene_num}: \")\n",
    "    print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_score_list)))\n",
    "    print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_score_list)))\n",
    "    print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_score_list)))\n",
    "    print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])))\n",
    "    print()\n",
    "\n",
    "    action_mean.append(numpy.mean(action_score_list))\n",
    "    object_mean.append(numpy.mean(object_score_list))\n",
    "    step_mean.append(numpy.mean(step_score_list))\n",
    "    mean_mean.append([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])\n",
    "\n",
    "print(\"Mean: \")\n",
    "print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_mean)))\n",
    "print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_mean)))\n",
    "print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_mean)))\n",
    "print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean(mean_mean)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KGRC4SI dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene 1: \n",
      "LCSscore(Action): 0.448\n",
      "LCSscore(Object): 0.429\n",
      "LCSscore(Step): 0.352\n",
      "LCSscore(Mean): 0.409\n",
      "\n",
      "Scene 2: \n",
      "LCSscore(Action): 0.520\n",
      "LCSscore(Object): 0.529\n",
      "LCSscore(Step): 0.430\n",
      "LCSscore(Mean): 0.493\n",
      "\n",
      "Scene 3: \n",
      "LCSscore(Action): 0.439\n",
      "LCSscore(Object): 0.396\n",
      "LCSscore(Step): 0.347\n",
      "LCSscore(Mean): 0.394\n",
      "\n",
      "Scene 4: \n",
      "LCSscore(Action): 0.472\n",
      "LCSscore(Object): 0.445\n",
      "LCSscore(Step): 0.371\n",
      "LCSscore(Mean): 0.429\n",
      "\n",
      "Scene 5: \n",
      "LCSscore(Action): 0.446\n",
      "LCSscore(Object): 0.407\n",
      "LCSscore(Step): 0.340\n",
      "LCSscore(Mean): 0.398\n",
      "\n",
      "Scene 6: \n",
      "LCSscore(Action): 0.439\n",
      "LCSscore(Object): 0.380\n",
      "LCSscore(Step): 0.330\n",
      "LCSscore(Mean): 0.383\n",
      "\n",
      "Scene 7: \n",
      "LCSscore(Action): 0.481\n",
      "LCSscore(Object): 0.493\n",
      "LCSscore(Step): 0.417\n",
      "LCSscore(Mean): 0.464\n",
      "\n",
      "Mean: \n",
      "LCSscore(Action): 0.464\n",
      "LCSscore(Object): 0.440\n",
      "LCSscore(Step): 0.369\n",
      "LCSscore(Mean): 0.424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action_mean = []\n",
    "object_mean = []\n",
    "step_mean = []\n",
    "mean_mean = []\n",
    "\n",
    "for scene_num in range(1, 8):\n",
    "\n",
    "    with open(f\"generated_kgrc_data/script_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the generated data file\n",
    "        result_scripts = json.load(json_file)\n",
    "\n",
    "    with open(f\"correct_kgrc_data/correct_scene{scene_num}.json\", \"r\") as json_file: #fill the path of the correct data file\n",
    "        correct_data = json.load(json_file)\n",
    "\n",
    "    step_score_list = []\n",
    "    action_score_list = []\n",
    "    object_score_list = []\n",
    "\n",
    "    for f_num, correct in correct_data.items():\n",
    "\n",
    "        evaluation_data = result_scripts.get(f_num)\n",
    "\n",
    "        for i, data in enumerate(evaluation_data):\n",
    "        \n",
    "            data = data.split()\n",
    "            new_data = [str(data[1])[1:-1]]\n",
    "\n",
    "            if len(data) == 4:\n",
    "                new_data.append(str(data[2])[1:-1]) \n",
    "            elif len(data) == 6:\n",
    "                new_data.append(str(data[2])[1:-1] + \"&\" + str(data[4])[1:-1])\n",
    "\n",
    "            evaluation_data[i] = new_data\n",
    "\n",
    "        for i, data in enumerate(correct):\n",
    "\n",
    "            data = data.split()\n",
    "            new_data = [str(data[0])[1:-1].lower()]\n",
    "\n",
    "            if len(data) == 3:\n",
    "                new_data.append(str(data[1])[1:-1]) \n",
    "            elif len(data) == 5:\n",
    "                new_data.append(str(data[1])[1:-1] + \"&\" + str(data[3])[1:-1])\n",
    "\n",
    "            correct[i] = new_data\n",
    "\n",
    "        step_score_list.append(lcs([' '.join(row) for row in evaluation_data], [' '.join(row) for row in correct]) / max(len(evaluation_data), len(correct)))\n",
    "        action_score_list.append(lcs([row[0] for row in evaluation_data], [row[0] for row in correct]) / max(len([row[0] for row in evaluation_data]), len([row[0] for row in correct])))\n",
    "        object_score_list.append(lcs([row[1] for row in evaluation_data if len(row) > 1], [row[1] for row in correct if len(row) > 1]) / max(len([row[1] for row in evaluation_data if len(row) > 1]), len([row[1] for row in correct if len(row) > 1])))\n",
    "\n",
    "    print(f\"Scene {scene_num}: \")\n",
    "    print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_score_list)))\n",
    "    print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_score_list)))\n",
    "    print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_score_list)))\n",
    "    print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])))\n",
    "    print()\n",
    "\n",
    "    action_mean.append(numpy.mean(action_score_list))\n",
    "    object_mean.append(numpy.mean(object_score_list))\n",
    "    step_mean.append(numpy.mean(step_score_list))\n",
    "    mean_mean.append([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])\n",
    "\n",
    "print(\"Mean: \")\n",
    "print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_mean)))\n",
    "print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_mean)))\n",
    "print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_mean)))\n",
    "print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean(mean_mean)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
