{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCSscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(a, b):\n",
    "    dp = [[0] * (len(b)+1) for i in range(len(a)+1)]\n",
    "\n",
    "    for i, vi in enumerate(a):\n",
    "        for j, vj in enumerate(b):\n",
    "            if vi == vj:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i+1][j], dp[i][j+1])\n",
    "\n",
    "    return dp[len(a)][len(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCSscore(Action): 0.501\n",
      "LCSscore(Object): 0.146\n",
      "LCSscore(Step): 0.112\n",
      "LCSscore(Mean): 0.253\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy\n",
    "\n",
    "with open(\"generated_vh_data/Object_name/object_name_script_scene7.json\", \"r\") as json_file: #fill the path of the generated data file\n",
    "    result_scripts = json.load(json_file)\n",
    "\n",
    "with open(\"correct_vh_data/correct_scene7.json\", \"r\") as json_file: #fill the path of the correct data file\n",
    "    correct_data = json.load(json_file)\n",
    "\n",
    "step_score_list = []\n",
    "action_score_list = []\n",
    "object_score_list = []\n",
    "\n",
    "for f_num, correct in correct_data.items():\n",
    "\n",
    "    evaluation_data = result_scripts.get(f_num)\n",
    "\n",
    "    for i, data in enumerate(evaluation_data):\n",
    "        \n",
    "        data = data.split()\n",
    "        new_data = [str(data[1])[1:-1]]\n",
    "\n",
    "        if len(data) == 4:\n",
    "            new_data.append(str(data[2])[1:-1]) \n",
    "        elif len(data) == 6:\n",
    "            new_data.append(str(data[2])[1:-1] + \"&\" + str(data[4])[1:-1])\n",
    "\n",
    "        evaluation_data[i] = new_data\n",
    "\n",
    "    for i, data in enumerate(correct):\n",
    "\n",
    "        data = data.split()\n",
    "        new_data = [str(data[0])[1:-1].lower()]\n",
    "\n",
    "        if len(data) == 3:\n",
    "            new_data.append(str(data[1])[1:-1]) \n",
    "        elif len(data) == 5:\n",
    "            new_data.append(str(data[1])[1:-1] + \"&\" + str(data[3])[1:-1])\n",
    "\n",
    "        correct[i] = new_data\n",
    "\n",
    "    step_score_list.append(lcs([' '.join(row) for row in evaluation_data], [' '.join(row) for row in correct]) / max(len(evaluation_data), len(correct)))\n",
    "    action_score_list.append(lcs([row[0] for row in evaluation_data], [row[0] for row in correct]) / max(len([row[0] for row in evaluation_data]), len([row[0] for row in correct])))\n",
    "    object_score_list.append(lcs([row[1] for row in evaluation_data if len(row) > 1], [row[1] for row in correct if len(row) > 1]) / max(len([row[1] for row in evaluation_data if len(row) > 1]), len([row[1] for row in correct if len(row) > 1])))\n",
    "\n",
    "print(\"LCSscore(Action): {:.3f}\".format(numpy.mean(action_score_list)))\n",
    "print(\"LCSscore(Object): {:.3f}\".format(numpy.mean(object_score_list)))\n",
    "print(\"LCSscore(Step): {:.3f}\".format(numpy.mean(step_score_list)))\n",
    "print(\"LCSscore(Mean): {:.3f}\".format(numpy.mean([numpy.mean(action_score_list), numpy.mean(object_score_list), numpy.mean(step_score_list)])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VH",
   "language": "python",
   "name": "vh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
